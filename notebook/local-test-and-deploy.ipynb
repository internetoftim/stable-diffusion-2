{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ca952c-38ce-46fd-891e-87371f38e6ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build, test and deploy a Stable Diffusion 2 endpoint in Paperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b395b1-ceb3-4a1d-9221-79278b048a3a",
   "metadata": {},
   "source": [
    "This notebook is a walkthrough of how to use the simple `api-deployment` repo to create a Docker image with a Stable Diffusion 2 endpoint which can be deployed locally and then in Paperspace. \n",
    "\n",
    "The container created using the cloned repository is also available in the Graphcore public registry on Dockerhub and can be used to directly launch a deployment locally or on Paperspace by skipping to Step 2 or 3 respectively. The minor changes to access and run the publicly available image rather than the locally built image from Step 1 will be outlined for each of these steps.\n",
    "\n",
    "Here, we'll cover:\n",
    "* Cloning and running up the FastAPI service from this notebook to create a locally hosted endpoint.\n",
    "* How to access and send requests to the endpoint and receive model output.\n",
    "* How to build a container to deploy the endpoint with Paperspace deployments, and access the model.\n",
    "\n",
    "The public model inference images available on Graphcore's Docker Hub have all of the necessary dependencies 'baked in', including executables and model binaries, to make the process of serving up an endpoint as smooth as possible. The internals of the image are based on the [api-deployment](https://github.com/graphcore/api-deployment) repository model-serving architecture. This is designed to be a straightforward example of serving a model with FastAPI and running up a local endpoint. Once you've tested your local endpoint functionality, you can use the same container to launch up a deployment in [Paperspace](https://www.paperspace.com/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da73df-40f6-4b9f-8793-0bdb37f10b1b",
   "metadata": {},
   "source": [
    "First, install all required dependencies for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee5ce4-ee50-461d-bf2a-eb06546ead37",
   "metadata": {},
   "outputs": [],
   "source": [
    "! # # Buildah installation:https://fabianlee.org/2022/08/02/buildah-installing-buildah-and-podman-on-ubuntu-20-04/\n",
    "! chmod +x setup.sh\n",
    "! ./setup.sh > /dev/null\n",
    "! buildah version\n",
    "\n",
    "! pip install gradient\n",
    "! pip install gradio\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c19355-0f98-4de7-80aa-bbe0e8b21a3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clone the repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c919ae4-6248-4e92-bb59-adba943b7f9e",
   "metadata": {},
   "source": [
    "First, clone the repository containing Stable Diffusion and the files for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cddbb5-df9c-414a-a3ec-9200413e0a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stable-diffusion'...\n",
      "remote: Enumerating objects: 176, done.\u001b[K\n",
      "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
      "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
      "remote: Total 176 (delta 86), reused 124 (delta 50), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (176/176), 636.90 KiB | 1.72 MiB/s, done.\n",
      "Resolving deltas: 100% (86/86), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/graphcore/stable-diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d0bc2-e64f-47b0-ad22-8a9d39a04263",
   "metadata": {},
   "source": [
    "The repo contains all of the basic necessities for serving an endpoint using stable diffusion. The `Dockerfile` specifies requirements and builds and runs the Docker container, `src` contains the model and model specific requirements, the FastAPI based endpoint for the model, and the model-independent files for creating the server. `run_server.sh` is a script to launch the server, which can be run directly or runs automatically as part of the docker container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba30ee4-a935-4e5f-857e-43f445397449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\n",
      "-rw-r--r-- 1 arsalanu all   576 May  9 11:15 docker-compose.yml\n",
      "-rw-r--r-- 1 arsalanu all   512 May  9 11:15 Dockerfile\n",
      "-rw-r--r-- 1 arsalanu all  1066 May  9 11:15 LICENSE\n",
      "-rw-r--r-- 1 arsalanu all   578 May  9 11:15 ps-deploy-config.yaml\n",
      "-rw-r--r-- 1 arsalanu all 10335 May  9 11:15 README.md\n",
      "-rw-r--r-- 1 arsalanu all    40 May  9 11:15 requirements.txt\n",
      "-rw-r--r-- 1 arsalanu all 19617 May  9 11:15 running_local_endpoint.ipynb\n",
      "-rwxr-xr-x 1 arsalanu all  2378 May  9 11:15 run_server.sh\n",
      "drwxr-xr-x 4 arsalanu all   112 May  9 11:15 src\n",
      "drwxr-xr-x 2 arsalanu all   146 May  9 11:15 utils\n"
     ]
    }
   ],
   "source": [
    "! cd stable-diffusion && ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91f04c-fa46-450b-93d2-cc0411f0fc5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run a local endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944105df-9fd9-46da-b8e6-f136d753a7be",
   "metadata": {
    "tags": []
   },
   "source": [
    "While building the container with all features/executables baked in is needed for launching a public Paperspace deployment, it is not a necessary step if you want to test and run the endpoint locally, as we can do this directly from the repo using the `run_server.sh` script. \n",
    "\n",
    "First, install all of the dependencies for serving the model, as well as for the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4791ed-5c6d-44d0-a244-0846db4cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd stable-diffusion && pip install -r requirements.txt && pip install -r src/models/stable_diffusion_2_txt2img_512/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51bf4d-0fb2-47b7-9270-60f6c28ad0f7",
   "metadata": {},
   "source": [
    "The terminal output when running the server is 'endless', and will block the above cell from ending, so for the purpose of the notebook we run the server as a background process. Once this command is run, the endpoint server will start warming up, performing any necessary preparation required to use the endpoint, such as building the model executables or creating and loading any other required binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37f6db-053e-45d8-8608-0a32b040ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_STORE_SYSTEM = get_ipython().system\n",
    "get_ipython().system = os.system # Allows running server processes in background in notebook\n",
    "\n",
    "! cd stable-diffusion && ./run_server.sh &\n",
    "\n",
    "get_ipython().system = _STORE_SYSTEM # Revert back to original iPython shell after starting server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f294c0e-706a-4298-926a-19faefa88936",
   "metadata": {},
   "source": [
    "In either instance, we need to wait for the server to be ready before actually sending any requests to the endpoint. We can wait for the built-in server health-check feature to return a positive status using a simple looping function. For Stable Diffusion, this step may take up to a few minutes. First, import the necessary packages for the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50c7a2-08d9-4dd0-bf4e-642bea1b22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567089f5-3b89-41d6-9459-c91993bd6e4f",
   "metadata": {},
   "source": [
    "Then we can instantiate our simple function which waits for the readiness status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05b6f0-e7b2-415d-b1a5-295b20d119df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_readiness(url):\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(f\"{url}/readiness\")\n",
    "            response = response.json()\n",
    "            if response['message'] == 'Readiness check succeeded.': \n",
    "                print(f\"Server ready - {response['message']}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Server waiting - {response['message']}\")\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562b5a0-6530-41af-b81c-c2cf6c051528",
   "metadata": {},
   "source": [
    "Next, we time and call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59642f5-c879-4496-887b-10362e03edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for readiness...\")\n",
    "\n",
    "warmup_start = time.perf_counter()\n",
    "ready = wait_for_readiness(\"http://0.0.0.0:8100\")\n",
    "\n",
    "print(f\"Warm up time: {time.perf_counter() - warmup_start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeaa49b-c633-40d9-bf72-0c468d60a3ca",
   "metadata": {},
   "source": [
    "The message should say 'Readiness check succeeded', which means we are ready to start generating images with the model using the live endpoint.\n",
    "\n",
    "Lets create a dictionary for the parameters to send to the model. This is specific to and defined by the model endpoint that has been created. For Stable Diffusion, we must pass:\n",
    "\n",
    "* `prompt`: Main body of text describing the image we want to create.\n",
    "* `random_seed`: Can be used to emulate a deterministic image output from the same prompt each time (we set this to random to observe variation in the image).\n",
    "* `guidance scale`: Specific to Stable Diffusion, it controls how strongly the generated image will follow the text output.\n",
    "* `return_json`: Defines whether to return a JSON object in the response or not, to receive an encoded image, we want to set this to True.\n",
    "* `negative_prompt`: Defines any aspects we don't want to see in the image.\n",
    "* `num_inference_steps`: The number of sampling steps undertaken by the model, increasing this up to a point should improve the image quality of the generated image, 25-50 steps is a reasonable range for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8abf76-b3e3-4675-9beb-204efcf3f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "      \"prompt\": \"big red dog\",\n",
    "      \"random_seed\": random.randint(0,99999999),\n",
    "      \"guidance_scale\": 9,\n",
    "      \"return_json\": True,\n",
    "      \"negative_prompt\": \"string\",\n",
    "      \"num_inference_steps\": 25\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6d975-111c-421f-8556-ae7700d6655e",
   "metadata": {},
   "source": [
    "Next, we can use `requests` to send a POST call to the REST endpoint at the IP address that the endpoint is running on. This will return an image in the response JSON body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed413ef-950e-4231-9245-dfc4eb705ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"http://0.0.0.0:8100/stable_diffusion_2_txt2img_512\", json=model_params)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(response.status_code)\n",
    "    \n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e6e3a7-36b4-4771-ba9c-8af639d45c1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, the image has been returned in Base64 encoded form within the JSON, we can decode this using the `base64` and `io` libraries to visualise the image. First, we decode the images returned by the model and convert them to PIL RGB images - in this case there is only one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f3a85-72fe-4122-ba77-2865421ee920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "images_b64 = [i for i in response['images']]\n",
    "\n",
    "pil_images = []\n",
    "for b64_img in images_b64:\n",
    "    base64bytes = base64.b64decode(b64_img)\n",
    "    bytesObj = io.BytesIO(base64bytes)\n",
    "    img = Image.open(bytesObj)\n",
    "    \n",
    "    pil_images.append(img)\n",
    "    \n",
    "print(\"Number of images returned: \", len(pil_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ea6c4-cb84-4112-8726-7aa167672d09",
   "metadata": {},
   "source": [
    "Finally, we can view the images with `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772cd2f-00db-45d4-9373-7e276bc5e929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(pil_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11088fe-fb78-4dd2-b2f1-bb94be4fcebc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy on Paperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fce5c6-79f7-41e3-962f-baecbd029696",
   "metadata": {},
   "source": [
    "To deploy on Paperspace, we need a container image which includes server files and model that we used previously to run the local endpoint. The container image for this example is already available in the Graphcore public registry and can be directly defined in the Paperspace deployment configuration, which will pull and run the image.\n",
    "\n",
    "Alternatively, it is also possible to build and push the container to your own Dockerhub registry. The address for the container can be used in the deployment configuration instead, and the deployment will run using your container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edea52-7804-471f-883b-9314c14848d2",
   "metadata": {},
   "source": [
    "### (Optional) Build and upload the image manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f57d5a-3a8e-4fb5-bdd8-44666f98fa41",
   "metadata": {},
   "source": [
    "In a local workspace, to build the container we can simply run:\n",
    "```\n",
    "docker build -t <local_container_name> .\n",
    "```\n",
    "From the root directory of the repository.\n",
    "\n",
    "As we are using the container within a Paperspace notebook VM from a container, it is preferable to use an alternative container manager to build the image with the available user privileges. For this purpose, we can use `buildah` and `podman` to run Docker-equivalent commands on a container. To build the container, we can use `buildah bud` rather than `Docker build` as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01b1da-0478-404f-8c2e-78f31741e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! buildah bud -t local-sd2-endpoint stable-diffusion/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1546a-d4e3-409a-8ba8-66c55df54594",
   "metadata": {},
   "source": [
    "Next, tag the image with the name of your Dockerhub registry and the name with which to upload the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c253eb-543d-444a-a75f-868ec58e4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = input(prompt=\"Enter your Dockerhub username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d3c6c-64c7-4120-a7a9-b55afea930f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = input(prompt=\"Enter an image name for the container to be uploaded to your Dockerhub registry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0f93b-c13e-40e5-9fd5-c9fbc972f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! buildah tag local-sd2-endpoint $username/$container_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81090b0c-3671-4f32-b4cf-181d4ab71433",
   "metadata": {},
   "source": [
    "Finally, push the built image to your personal Dockerhub registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b33d17-70fa-4c0e-a6ac-f2a5409e53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "! buildah push docker.io://$username/$container_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5717f6-7b8e-4ccc-bf86-a2a8dec681bd",
   "metadata": {},
   "source": [
    "Now that we have tested the deployment using our local endpoint, we can create a full deployment on IPUs in Paperspace. For this stage, we require a built container serving an endpoint as created in the previous steps. The essential feature of deployment is to create a Paperspace deployment specification `.yaml` which contains the necessary information to launch the deployment. In our `stable-diffusion` repo, there is a ready-to-go specification `ps-deploy-config.yaml` which can be used to generate a deployment. Lets have a look at the contents of this spec:\n",
    "\n",
    "```\n",
    "enabled: true\n",
    "image: gcapidev/stable-diffusion-2-512-deployment\n",
    "port: 8100\n",
    "env:\n",
    "  - name: SERVER_MODELS\n",
    "    value: '[{\"model\":\"stable_diffusion_2_txt2img_512\", \"replicas\":\"2\"}]'\n",
    "  - name: POPTORCH_CACHE_DIR\n",
    "    value: /src/model_cache\n",
    "  - name: HUGGINGFACE_HUB_CACHE\n",
    "    value: /src/model_cache\n",
    "  - name: HF_HOME\n",
    "    value: /src/model_cache\n",
    "resources:\n",
    "  replicas: 1\n",
    "  instanceType: Bow-POD16\n",
    "  autoscaling:\n",
    "    enabled: true\n",
    "    maxReplicas: 2\n",
    "    metrics:\n",
    "      - metric: requestDuration\n",
    "        summary: average\n",
    "        value: 2\n",
    "healthChecks:\n",
    "  readiness:\n",
    "    path: /readiness\n",
    "```\n",
    "\n",
    "The key things to specify in the file are:\n",
    "* `image`: the address of the container to be deployed on Dockerhub in the format `<username>/<container_name>`. For example, if deploying from the Graphcore public registry, the username will be `graphcore`. \n",
    "* `SERVER_MODELS` environment variable: \n",
    "    * `\"model\"`: A container may have multiple model endpoints within it, defining this variable in the config allows you to specify which of the models you want to start a deployment with.\n",
    "    * `\"replicas\"`: This defines the maximum number of replicas the model can create within a single machine when under load (internal autoscaling) e.g., if the model uses 8 IPUs and you are deploying on a number of IPU-POD16s, each POD16 will be able to launch up to 2 instances of the model if the replicas are set to 2. This allows for internal scaling (within machine) as well as the existing external scaling (over multiple machines).\n",
    "* `replicas` in `resources`: This is the number of IPU machines the endpoint is replicated over.\n",
    "* `instanceType`: Which IPU machine to use, available options include an IPU-POD4, IPU-POD16 or Bow-POD16.\n",
    "* `autoscaling`: Allows you to set maximum replicas to *externally* scale over, and the metrics to use to determine when to scale, in the above case, we increase the number of replicas if the duration of request responses exceeds 2 seconds.\n",
    "\n",
    "**NOTE: By default, this notebook will use the Graphcore public registry image for Stable Diffusion - if you would like to deploy the custom image uploaded to your personal registry, open the `ps-deploy-config.yaml` from the cloned repository from the folder tree on the left, and modify the `image` field to point to your container.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccf042-b847-4669-be7c-2181c8f07dd6",
   "metadata": {},
   "source": [
    "### Launch the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fea7a7-0cdf-4ae0-8adc-651053c6a52b",
   "metadata": {},
   "source": [
    "Before deploying, ensure you have an active project on the Paperspace console which you will be deploying from. Then, we can use the Gradient CLI to deploy directly from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41d41e-2619-4abc-b683-2fc5ffe3cff0",
   "metadata": {},
   "source": [
    "In the Paperspace console, generate an API key while logged in to your account from under `Team settings`. Use this API key to log in to your account from the Gradient CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c039ea24-b3cd-47ea-80ed-53ea14be9b87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m apiKey \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnter your Gradient API key:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1172\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1173\u001b[0m     )\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localdata/arsalanu/popsdk_venvs/3.2.0+1277/3.2.0+1277_poptorch/lib/python3.8/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "TOKEN = getpass.getpass(prompt='Enter your Gradient API key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4c5bdd-2e39-4913-a604-169377bbe7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value\n"
     ]
    }
   ],
   "source": [
    "! gradient apiKey $TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f0216-7f5c-4a4c-a7d3-be0ea2db81ad",
   "metadata": {},
   "source": [
    "Next, check which clusters contain IPU machines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da62907-49a1-4f58-9df4-69c02e5555ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m+-----------+-----------+-----------+--------------+-----------+-----------------+-----------+[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "| Name      | Kind      | CPU Count | RAM [Bytes]  | GPU Count | GPU Model       | Clusters  |\n",
      "+-----------+-----------+-----------+--------------+-----------+-----------------+-----------+\n",
      "| A100      | a100      | 12        | 96636764160  | 1         | Ampere A100     | clg07azjl |\n",
      "| A100-80G  | a100-80g  | 12        | 96636764160  | 1         | Ampere A100 80G | clg07azjl |\n",
      "| A4000     | a4000     | 8         | 48318382080  | 1         | Ampere A4000    | clg07azjl |\n",
      "| A4000x2   | a4000     | 16        | 96636764160  | 2         | Ampere A4000    | clg07azjl |\n",
      "| A5000     | a5000     | 8         | 48318382080  | 1         | Ampere A5000    | clg07azjl |\n",
      "| A5000x2   | a5000     | 16        | 96636764160  | 2         | Ampere A5000    | clg07azjl |\n",
      "| A6000     | a6000     | 8         | 48318382080  | 1         | Ampere A6000    | clg07azjl |\n",
      "| A6000x2   | a6000     | 16        | 96636764160  | 2         | Ampere A6000    | clg07azjl |\n",
      "| Bow-POD16 | bow-pod16 | 208       | 455266533376 | 0         | N/A             | clehbtvty |\n",
      "| C4        | cpu       | 2         | 4294967296   | 0         | N/A             | clg07azjl |\n",
      "| C5        | cpu       | 4         | 8589934592   | 0         | N/A             | clg07azjl |\n",
      "| C7        | cpu       | 12        | 32212254720  | 0         | N/A             | clg07azjl |\n",
      "| IPU-POD16 | ipu-pod16 | 208       | 455266533376 | 0         | N/A             | clehbtvty |\n",
      "| P4000     | p4000     | 8         | 32212254720  | 1         | Quadro P4000    | clg07azjl |\n",
      "| P5000     | p5000     | 8         | 32212254720  | 1         | Quadro P5000    | clg07azjl |\n",
      "| P6000     | p6000     | 8         | 32212254720  | 1         | Quadro P6000    | clg07azjl |\n",
      "| RTX4000   | rtx4000   | 8         | 32212254720  | 1         | Quadro RTX4000  | clg07azjl |\n",
      "| V100      | v100      | 8         | 32212254720  | 1         | Tesla V100      | clg07azjl |\n",
      "| V100-32G  | v100-32g  | 8         | 32212254720  | 1         | Tesla V100 32G  | clg07azjl |\n",
      "+-----------+-----------+-----------+--------------+-----------+-----------------+-----------+\u001b[0m\n",
      "\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "! gradient clusters machineTypes list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af75e56-1893-4e92-8749-efcb16761e08",
   "metadata": {},
   "source": [
    "Deploy on paperspace using `gradient deployments create` with the arguments:\n",
    "* `--name`: the desired name for your deployment\n",
    "* `--projectId`: Obtain your project ID from your project (this is on the project page on Paperspace and also printed when a project is created from the terminal)\n",
    "* `--spec`: Define the specification file\n",
    "* `--clusterId`: Obtain the cluster ID from Step 2\n",
    "    \n",
    "Lets set these values and create the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d88f816-c285-4561-a4e6-f62fc4aa69fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a name for your deployment: aa\n"
     ]
    }
   ],
   "source": [
    "deployment_name = input(prompt='Enter a name for your deployment:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e82048b9-22bd-431d-a5b0-65e7e46a0efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Paperspace project ID: jjn\n"
     ]
    }
   ],
   "source": [
    "project_id = input(prompt='Enter your Paperspace project ID:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90951f6a-1a30-45ff-b402-d2f07e23c17d",
   "metadata": {},
   "source": [
    "Then launch the deployment with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c28ee7-b783-4f28-9d0e-f9843ae23537",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gradient deployments create \\\n",
    "    --name $deployment_name \\\n",
    "    --projectId $project_id \\\n",
    "    --spec \"./stable-diffusion/ps-deploy-config.yaml\" \\\n",
    "    --clusterId \"clehbtvty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca00c9f-dd56-4080-acef-b7d406e9adc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "This will return a unique deployment ID for your deployment. You can view the spec, URL, and deployment run status with:\n",
    "```\n",
    "gradient deployments get --id <your_deployment_id>\n",
    "```\n",
    "\n",
    "You can also view current metrics, logs and deployment status in the Paperspace console, inside your created project by switching to the *Deployments* tab and clicking on the running deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06506d1f-2b3c-44c4-83d0-a9830e6d1479",
   "metadata": {},
   "source": [
    "The URL is the address you should use to request the endpoint. The process for sending requests is the same as with `localhost` but requires simply replacing `localhost` with the Gradient deployment URL generated for your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a40f82-2321-417f-8d80-e1e37213ccf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your deployment ID: mnj\n"
     ]
    }
   ],
   "source": [
    "your_deployment_id = input(prompt='Enter your deployment ID:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094e8a5-4977-44cf-9cbe-6057b5ab381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gradient deployments get --id $your_deployment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727ba90-ed19-4802-9737-b399c11ba10f",
   "metadata": {},
   "source": [
    "You can update the deployment, for example to change the spec or stop the deployment with:\n",
    "```\n",
    "gradient deployments update \n",
    "    --id <your_deployment_id>\n",
    "    --name <your_deployment_name>\n",
    "    --projectId <your_project_id>\n",
    "    --spec <updated_deployment_config_spec>\n",
    "    --clusterId <cluster_id>\n",
    "```\n",
    "To stop the deployment, update the spec with the `enabled` value set to `false`.\n",
    "You can also change the environment variables in the spec by modifying the `SERVER_MODELS` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1af75-3b1a-4e71-98a7-691a3677d2ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Optional) Create a simple demo frontend for your deployment with **Gradio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f1ba6-cff2-4853-9699-f32f3321fb62",
   "metadata": {},
   "source": [
    "You can create a create an easy frontend demo for your deployment using Gradio. All you need is the URL of your deployment, and a simple function to process the Stable Diffusion 2 input parameters, request the model and decode the output image - the same process we set out for the local endpoint earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54a67a-a2fc-4f89-b124-e9e4cfd96e94",
   "metadata": {},
   "source": [
    "This notebook points by default to the locally hosted endpoint. If you would like to run the Gradio app with the launched Paperspace deployment, change the following cell to point to the generated Paperspace deployment URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580eea7-d257-416a-9d8f-658765d5a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://0.0.0.0:8100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086dd24-64b8-42c0-83c8-b5115a833bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "def stable_diffusion_2_inference(prompt, guidance_scale, num_inference_steps):\n",
    "    model_params = {\n",
    "      \"prompt\": prompt,\n",
    "      \"random_seed\": random.randint(0,99999999),\n",
    "      \"guidance_scale\": guidance_scale,\n",
    "      \"return_json\": True,\n",
    "      \"negative_prompt\": \"string\",\n",
    "      \"num_inference_steps\": num_inference_steps\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{URL}/stable_diffusion_2_txt2img_512\", json=model_params)\n",
    "    response = response.json()\n",
    "    \n",
    "    images_b64 = [i for i in response['images']]\n",
    "    pil_images = []\n",
    "    for b64_img in images_b64:\n",
    "        base64bytes = base64.b64decode(b64_img)\n",
    "        bytesObj = io.BytesIO(base64bytes)\n",
    "        img = Image.open(bytesObj)\n",
    "\n",
    "        pil_images.append(img)\n",
    "    \n",
    "    return np.array(pil_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261d1eb-1558-47ca-933c-b185884105e6",
   "metadata": {},
   "source": [
    "Then, we can initialise the Gradio app to launch a GUI from inside this notebook by defining our inputs, outputs and the processing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d9a00-34bb-4408-bc59-3991df11aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n",
    "demo = gr.Interface(\n",
    "    fn=stable_diffusion_2_inference, \n",
    "    inputs=[gr.Textbox(value=\"Ice skating on the moon\"),\n",
    "            gr.Slider(1,50,value=9, step=1, label='Guidance scale'),\n",
    "            gr.Slider(1,100,value=25, step=1, label='Number of steps')\n",
    "           ], \n",
    "    outputs=gr.Image(shape=(512,512))\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f93824-5cce-4395-b299-926c096b86c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deleting the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410a4a7-aafb-464c-b652-5c1d4953a402",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, to delete your deployment completely, simply run the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab5cc7-6eab-4abc-8b4e-c7ae8d1524c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient deployments delete --id $your_deployment_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
